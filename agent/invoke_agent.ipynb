{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from backend.agent import workflow, AgentState\n",
    "from uuid import uuid4\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "result = None\n",
    "config = {\"configurable\": {\"thread_id\": uuid4()}}\n",
    "default = AgentState()\n",
    "default.messages.append(\n",
    "    HumanMessage(content=\"Create 3 posts from linkedin profile shreyshahh\")\n",
    ")\n",
    "\n",
    "default.generated_posts= [\n",
    "    \"Tracking token usage in Azure ChatOpenAI is a game-changer for developers.\\n\\nHere's why ↓\\n\\nIn the fast-paced world of AI and machine learning, managing resources is crucial. Developers using Azure ChatOpenAI with Langchain PromptTemplate in Python face a common challenge: tracking token usage and calculating costs.\\n\\nBut there's a solution.\\n\\n→ Langfuse: Host it with Docker for seamless token tracking.\\n→ Langsmith: Offers similar capabilities for insights into token consumption.\\n\\nThese tools are essential for optimizing performance and cost-efficiency. They help developers monitor usage, make informed scaling decisions, and manage costs effectively.\\n\\nThe developer community is buzzing with shared insights and solutions. This collaboration fosters innovation and efficiency in AI development.\\n\\nBy integrating these tools, developers can achieve better performance and cost savings. It's not just about individual projects; it's about advancing AI technologies as a whole.\\n\\nFor more detailed guidance, check out [Langfuse's integration guide](https://langfuse.com/guides/cookbook/integration_azure_openai_langchain).\\n\\n♻️ Repost this if you think it's the future.\\n\\nPS: If you want to stay updated with genAI\\n\\n1. Scroll to the top.\\n2. Follow Shrey Shah to never miss a post.\\n\\n#AI #MachineLearning #Azure #TokenTracking #Langchain #Innovation #CostEfficiency\",\n",
    "    \"RAG pipelines are the future of AI content creation.\\n\\nHere's why they're a game-changer:\\n\\n→ They combine retrieval systems with generative models.\\n→ They enhance accuracy and relevance in AI-generated content.\\n→ They require seamless integration of diverse data sources.\\n\\nBut let's be real:\\n\\nBuilding RAG pipelines isn't easy.\\n\\nChallenges include:\\n- Integrating data from multiple repositories.\\n- Maintaining data integrity and consistency.\\n- Fine-tuning generative models for contextually relevant outputs.\\n\\nYet, innovation is on our side.\\n\\nAdvances in NLP and machine learning are paving the way:\\n- Transfer learning and reinforcement learning are boosting model performance.\\n- Open-source tools are democratizing access to cutting-edge tech.\\n\\nThe potential is immense.\\n\\nBy tackling these challenges, we can transform data interaction and leverage AI like never before.\\n\\n♻️ Repost this if you think it's the future.\\n\\nPS: If you want to stay updated with genAI\\n\\n1. Scroll to the top.\\n2. Follow Shrey Shah to never miss a post.\\n\\n#AI #DataScience #MachineLearning #Innovation #RAGpipelines\"\n",
    "  ]\n",
    "while True:\n",
    "    result = await workflow.ainvoke(input=result if result else default, config=config)\n",
    "    next = input(result[\"messages\"][-1].content)\n",
    "    result[\"messages\"].append(HumanMessage(content=next))\n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
